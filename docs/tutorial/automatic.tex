\subsection{Automatic Differentiation}
\label{subsec:automatic}

\begin{frame}
\frametitle{Automatic Differentiation}
\footnotesize
\colorbox{green!10}{\vbox{
\begin{quote}
{\color{brown}
%
Algorithmic/Automatic Differentiation (AD) uses the software representation of
a function to obtain an efficient method for calculating its derivatives.  
%
These derivatives can be of arbitrary order and are analytic in nature (do not
have any truncation error).
}\hfill\textbf{\alert{B. Bell -- author of \texttt{CppAD}}}
\end{quote}
}}
%
\normalsize
\begin{itemize}
  \item No truncation error --- like Symbolic, unlike Numerical.
  \item Gives an algorithm for computing the derivative.
    \begin{itemize}
      \item Often, (conveniently) in terms of the consitituent functions.
    \end{itemize}
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Automatic Scalar Differentiation}
%\framesubtitle{Symbolic Differentiation Comparison}
%
%\begin{center}
%\vspace{-40pt}
%\colorbox{green!10}{\vbox{
%Algorithmic $\approx$ Symbolic, but is more suited for computers as it gives
%an algorithm to compute the derivate.
%}}
%\end{center}
%%
%Consider the function $\color{m1}f(x)=x^3$
%%
%\begin{itemize}
%  \item Symbolic: we apply the power rule to get $\color{m1}f(x)=3x^2$
%  \item Algorithmic: we see $\color{m1}f(x)$ as $\color{m1}=g(g(x,x),x)$
%    \tiny
%    \begin{enumerate}
%      \item %Product/chain rule: 
%        $\color{m1}f'(x)=g\left(\frac{d(g(x,x))}{dx},x\right) + 
%                         g\left(\frac{dx}{dx},g(x,x)\right)$
%      \item %Product/chain rule: 
%        $\color{m1}f'(x)=g\left(g\left(x,\frac{dx}{dx}\right)+
%                                g\left(\frac{dx}{dx},x\right),x\right) + 
%                         g\left(\frac{dx}{dx},g\left(x,x\right)\right)$
%      \item %Reduction: 
%        $\color{m1}f'(x)=g\left(g\left(x,1\right)+
%                         g\left(1,x\right),x\right) + 
%                         g\left(1,g\left(x,x\right)\right)$
%      \item %Reduction: 
%        $\color{m1}f'(x)=(x+x)*x + x^2 = 2x^2 + x^2 = 3x^2$
%    \end{enumerate}
%    \normalsize
%\end{itemize}
%\end{frame}

\begin{frame}
\frametitle{Automatic Scalar/Vector Differentiation}
\framesubtitle{Forward and Reverse Mode} 
%
\centerline{Compute $\color{m1}f'(x); f(x)=x^3$}
%
\begin{center}
\begin{tikzpicture}
[scale=.7,transform shape,
matrix/.style={rectangle,draw=black,fill=blue!20,thick},
operation/.style={circle,draw=black,fill=red!20,thick},
arrow/.style={<-,thick}]

\node[matrix] (x1) {$x$};
\node[matrix, right=2cm of x1] (x2) {$x$};

\node[left=0.5cm of x1] (lx1) {};
\node[left=0.5cm of x2] (lx2) {};

\node[right=1cm of x1] (x1x2) {};
\node[right=.5cm of x2] (x2x3) {};

\node[operation, above=1.5cm of x1x2] (f1) {$\times{}$} 
edge [arrow] (x2) edge [arrow] (x1);

\node[matrix, right=3cm of f1] (x3) {$x$};
\node[left=0.5cm of x3] (lx3) {};

\node[operation, above=3cm of x2x3] (f2) {$\times{}$}
edge [arrow] (x3) edge [arrow] (f1);

\node[matrix, above=1.2cm of f2] (f) {$f(x)=x^3$} edge [arrow] (f2);

\node[left,align=center] (textf1) at (f1.west){$\color{blue}t_1=x^2$};
\node[left,align=center] (textf2) at (f2.west){$\color{blue}t_2=xt_1$};
\node[left,align=center] (textf) at (f.west) {$\color{blue}f=t_2$};

\onslide<2-5>{
\node[left,align=center] (textx1) at (x1.west) {$\color{red}1$};
\node[left,align=center] (textx2) at (x2.west) {$\color{red}1$};
\node[left,align=center] (textx3) at (x3.west) {$\color{red}1$};
}
\onslide<3-5>{
\node[above=1.5pt of textf1, align=center] (textd1) 
                                          {$\color{red}t_1'=x+x$};
}
\onslide<4-5>{
\node[above=1.5pt of textf2, align=center] (textd2)
                                          {$\color{red}t_2'=xt_1'+t_1$};
}
\onslide<5>{
  \node[above=1.2pt of textf,align=center] (textd) {$\color{red}f'=t_2'$};
}

\onslide<6>{
  \node[above=1.2pt of textf,align=center] (textr) 
            {$\color{magenta}r=0, i=1$};
}
\onslide<7-10>{
\node[above=1.2pt of textf2,align=center] (textr2) 
                                  {$\color{magenta}r=0,i=1$};
}
\onslide<8-10>{
\node[above=1.2pt of textf1,align=center] (textr1) 
                                  {$\color{magenta}r=0,i=x$};
}
\onslide<8>{
\node[right=0.2cm of x3] (textr3) {$\color{magenta}r=0,i=t_1$};
}

\onslide<9>{
\node[left=0.2cm of x1] (textr1) {$\color{magenta}r=0,i=x^2$};
\node[right=0.2cm of x2] (textr2) {$\color{magenta}r=0,i=x^2$};
\node[right=0.2cm of x3] (textr3) {$\color{cyan}r=t_1$};
}

\onslide<10>{
\node[left=0.2cm of x1] (textr1) {$\color{cyan}r=x^2$};
\node[right=0.2cm of x2] (textr2) {$\color{cyan}r=x^2$};
\node[right=0.2cm of x3] (textr3) {$\color{cyan}r=x^2$};
}

\end{tikzpicture}

\onslide<2-5>{\begin{center}\textcolor{blue}
                    {Forward Mode: Inside-out Application}\end{center}}
\vspace{-30pt}
\onslide<6-10>{\begin{center}\textcolor{magenta}
                    {Reverse Mode: Outside-in Application}\end{center}}

\end{center}
\end{frame}

\begin{frame}
\frametitle{Automatic Scalar/Vector Differentiation}
\framesubtitle{Notes on Reverse Mode Differentiation}
%
Reverse Mode Differentiation (1971):
%
\begin{itemize}
  \item Chain rule applied in reverse order of function evaluation.
  \item Same technique as:
    \begin{itemize}
      \item Back-propagation algorithm (1969) for neural networks.
      \item Forward-backward algorithm for training HMMs (1970).
    \end{itemize}
  \item Applies to very general classes of functions.
\end{itemize}
%
\end{frame}

\begin{frame}
\frametitle{Automatic Scalar/Vector Differentiation}
\framesubtitle{Computational Cost of Differentiation}
\colorbox{green!10}{\vbox{
\begin{quote}
{\color{brown}Under quite realistic assumptions the evaluation of a gradient
requires never more than five times the effort of evaluating the
underlying function by itself.}\\
\hfill\textbf{\alert{Andreas Griewank (1988)}}
\end{quote}
}}
\end{frame}
